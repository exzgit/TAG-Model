{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DxfQMoRH84A"
      },
      "source": [
        "\n",
        "# **Text Assistant Generative Model (TAGModel)**\n",
        "---\n",
        "## **Introduction**\n",
        "Text generation is a fascinating field within natural language processing (NLP) that involves creating coherent and contextually relevant text based on given input. In this notebook, we'll explore how to build a text generation model using a Transformer architecture, a powerful deep learning model known for its effectiveness in handling sequential data.\n",
        "\n",
        "\n",
        "\n",
        "## **Importing Libraries**\n",
        "We start by importing the necessary libraries and modules required for building and training our Transformer model. These include TensorFlow, NumPy, Matplotlib, and other relevant components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJoi2KAvIjYp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADe2xBTCImw3"
      },
      "source": [
        "## **Scaled Dot-Product Attention Mechanism**\n",
        "The scaled dot-product attention mechanism is a key component of the Transformer architecture. It computes the dot product of the query and key vectors, scales it, applies a softmax function to obtain attention weights, and finally computes a weighted sum of the value vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlKWXvdAIq-X"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask=None):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_product, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, values)\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnoWq3nQIt39"
      },
      "source": [
        "## **Multi-Head Attention Layer**\n",
        "The multi-head attention layer allows the model to focus on different parts of the input sequence independently by splitting the query, key, and value vectors into multiple heads and then concatenating the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cye0LDp3IxhD"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, head_size, dropout=0.1):\n",
        "        super(MultiHeadAttentionLayer, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.depth = self.head_size // self.num_heads\n",
        "        self.wq = Dense(self.head_size)\n",
        "        self.wk = Dense(self.head_size)\n",
        "        self.wv = Dense(self.head_size)\n",
        "        self.dense = Dense(self.head_size)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        q, k, v, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        query = self.wq(q)\n",
        "        key = self.wk(k)\n",
        "        value = self.wv(v)\n",
        "\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(query, key, value, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.head_size))\n",
        "\n",
        "        outputs = self.dense(concat_attention)\n",
        "        return outputs, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlPDTLqXI0fP"
      },
      "source": [
        "## **Positional Encoding**\n",
        "Since the Transformer model doesn't inherently understand the order of the sequence, positional encoding is added to provide information about the position of each token in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXWvX5UHI6rL"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(max_len, head_size):\n",
        "    pos = tf.cast(tf.range(max_len)[:, tf.newaxis], dtype=tf.float32)\n",
        "    i = tf.cast(tf.range(head_size)[tf.newaxis, :], dtype=tf.float32)\n",
        "    angle_rads = pos / tf.pow(10000, (i-i%2)/head_size)\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "    return tf.concat([sines, cosines], axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHNipyIVI9Zd"
      },
      "source": [
        "## **Transformer Encoder Layer**\n",
        "The transformer encoder layer consists of a multi-head self-attention mechanism followed by a position-wise feed-forward neural network. It applies layer normalization and residual connections around each sub-layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huJQEco5JAzC"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder_layer(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    attention, _ = MultiHeadAttentionLayer(num_heads, head_size)(inputs={'query': inputs, 'key': inputs, 'value': inputs, 'mask': None})\n",
        "    attention = Dropout(dropout)(attention)\n",
        "    attention = LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    ffn = tf.keras.Sequential(\n",
        "        [Dense(ff_dim, activation=\"relu\"), Dense(inputs.shape[-1]),]\n",
        "    )\n",
        "    ffn_out = ffn(attention)\n",
        "    ffn_out = Dropout(dropout)(ffn_out)\n",
        "    return LayerNormalization(epsilon=1e-6)(attention + ffn_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uklyzePyJCzH"
      },
      "source": [
        "## **Building the Transformer Model**\n",
        "We build the transformer model by stacking multiple transformer encoder layers. This model takes tokenized input sequences, embeds them, and passes them through the encoder layers to generate output sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7g2BFX2JCTh"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, num_layers, dropout=0):\n",
        "    inputs += positional_encoding(tf.shape(inputs)[1], head_size)\n",
        "    x = inputs\n",
        "    for _ in range(num_layers):\n",
        "        x = transformer_encoder_layer(x, head_size, num_heads, ff_dim, dropout)\n",
        "    return x\n",
        "\n",
        "def build_transformer_model(max_len, vocab_size, head_size, num_heads, ff_dim, num_layers, dropout=0.5):\n",
        "    inputs = Input(shape=(max_len,))\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=head_size)(inputs)\n",
        "    x = transformer_encoder(embedding_layer, head_size, num_heads, ff_dim, num_layers, dropout)\n",
        "    x = Dense(vocab_size, activation=\"softmax\")(x[:, -1, :])\n",
        "    return Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoNO8na4JI3B"
      },
      "source": [
        "## **Loading and Preprocessing the Dataset**\n",
        "We load and preprocess the dataset by tokenizing the text data and generating input sequences for training the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlaS5TC3JLl-"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"TAGModel/dataset/\"\n",
        "conversations = []\n",
        "for file_name in os.listdir(dataset_path):\n",
        "    if file_name.endswith('.txt'):\n",
        "        file_path = os.path.join(dataset_path, file_name)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            conversations.append(file.read())\n",
        "\n",
        "conversations = [line for convo in conversations for line in convo.splitlines()]\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=False, filters='', lower=False, oov_token=\"[-START-] [-END-]\")\n",
        "tokenizer.fit_on_texts(conversations)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "tokenizer_path = \"TAGModel/models/tokenizer.pkl\"\n",
        "with open(tokenizer_path, 'wb') as tokenizer_file:\n",
        "    pickle.dump(tokenizer, tokenizer_file)\n",
        "\n",
        "input_sequences = []\n",
        "for line in conversations:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "X, y = input_sequences[:,:-1], input_sequences[:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zfNXdL3JMn9"
      },
      "source": [
        "## **Training the Model**\n",
        "With the dataset prepared, we train our Transformer model using the compiled model, optimizer, loss function, and training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5buClFeHJYOx"
      },
      "outputs": [],
      "source": [
        "head_size = 720\n",
        "num_heads = 8\n",
        "ff_dim = 2045\n",
        "num_layers = 14\n",
        "dropout = 0.2\n",
        "\n",
        "checkpoint_path = \"TAGModel/checkpoint/\"\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model = build_transformer_model(max_sequence_len - 1, total_words, head_size, num_heads, ff_dim, num_layers, dropout)\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X, y, batch_size=245, epochs=50, validation_split=0.3, verbose=1, callbacks=[checkpoint_callback])\n",
        "\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "model.save(\"TAGModel/models/TAGModel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS_F9tyVJbpl"
      },
      "source": [
        "## **Generating Text**\n",
        "After training, we use the trained model to generate text based on a given input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91xgtd51Jgi3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict function\n",
        "def generate_text(model, tokenizer, initial_text, temperature=1.0, max_length=1024):\n",
        "    input_text = initial_text + \" [-START-]\"\n",
        "    output_text = \"[-START-] \"\n",
        "    for _ in range(max_length):\n",
        "        tokenized_input = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        padded_input = pad_sequences([tokenized_input], maxlen=max_sequence_len-1, padding='pre')\n",
        "        response = model.predict(padded_input)[0]\n",
        "\n",
        "        # Adjust predictions with temperature scaling\n",
        "        response = np.log(response) / temperature\n",
        "        exp_response = np.exp(response)\n",
        "        response = exp_response / np.sum(exp_response)\n",
        "\n",
        "        # Sample the next token\n",
        "        if output_text.startswith('[-START-]'):\n",
        "            predicted_word_index = np.random.choice(len(response), p=response)\n",
        "            predicted_word = tokenizer.index_word[predicted_word_index]\n",
        "        \n",
        "        if predicted_word == '[-END-]':\n",
        "            break\n",
        "\n",
        "        input_text += ' ' + predicted_word\n",
        "        output_text += predicted_word + ' '      \n",
        "\n",
        "    output_text = output_text.replace(\"[-START-] \", \"\")\n",
        "\n",
        "\n",
        "    return output_text\n",
        "\n",
        "# Example usage\n",
        "user_input = \"Halo! Apa kabar?\"\n",
        "response = generate_text(model=model, tokenizer=tokenizer, initial_text=user_input, temperature=0.3)\n",
        "print(\"\\033[92m\" + \"GENERATE >> \" + \"\\033[0m\" + \" \" + \"\\033[96m\" + response + \"\\033[0m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ciiVDBJkPw"
      },
      "source": [
        "## **Visualizing Training History**\n",
        "Lastly, we visualize the training history of our model using scatter plots, which display the loss and accuracy metrics over each epoch of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uez8TudbJjGb"
      },
      "outputs": [],
      "source": [
        "plt.scatter(range(1, len(history.history['loss']) + 1), history.history['loss'], label='Training Loss')\n",
        "plt.scatter(range(1, len(history.history['val_loss']) + 1), history.history['val_loss'], label='Validation Loss')\n",
        "plt.scatter(range(1, len(history.history['accuracy']) + 1), history.history['accuracy'], label='Training Accuracy')\n",
        "plt.scatter(range(1, len(history.history['val_accuracy']) + 1), history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.title('Training History')\n",
        "plt.legend()\n",
        "plt.savefig(\"TAGModel/image/1.jpeg\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hsw8-4vJoZd"
      },
      "source": [
        "## **Conclusion**\n",
        "\n",
        "In this notebook, we've explored the implementation of a Transformer-based text generation model using TensorFlow and Keras. Let's recap the key functions and their roles in the model:\n",
        "\n",
        "1. **Scaled Dot-Product Attention Mechanism**: This function computes the scaled dot product of the query and key vectors, applies a softmax function to obtain attention weights, and computes a weighted sum of the value vectors. It allows the model to focus on different parts of the input sequence based on their importance.\n",
        "\n",
        "2. **Multi-Head Attention Layer**: The multi-head attention layer splits the input into multiple heads, computes attention independently, and then concatenates the results. This mechanism enables the model to capture different aspects of the input sequence simultaneously.\n",
        "\n",
        "3. **Positional Encoding**: Since the Transformer model lacks inherent understanding of sequence order, positional encoding is added to provide positional information to the model. It helps the model learn the sequential relationships between tokens in the input sequence.\n",
        "\n",
        "4. **Transformer Encoder Layer**: This layer consists of a multi-head self-attention mechanism followed by a position-wise feed-forward neural network. It applies layer normalization and residual connections around each sub-layer, allowing the model to effectively capture and process sequential information.\n",
        "\n",
        "5. **Building the Transformer Model**: We stack multiple transformer encoder layers to build the complete Transformer model. The model takes tokenized input sequences, embeds them, and passes them through the encoder layers to generate output sequences.\n",
        "\n",
        "6. **Loading and Preprocessing the Dataset**: We load and preprocess the dataset by tokenizing the text data and generating input sequences for training the model.\n",
        "\n",
        "7. **Training the Model**: With the dataset prepared, we train our Transformer model using the compiled model, optimizer, loss function, and training data. We monitor the training progress and save the best weights using callbacks.\n",
        "\n",
        "8. **Generating Text**: After training, we use the trained model to generate text based on a given input sequence. The model predicts the next token in the sequence iteratively, generating coherent and contextually relevant text.\n",
        "\n",
        "9. **Visualizing Training History**: Lastly, we visualize the training history of our model using scatter plots. These plots display the loss and accuracy metrics over each epoch of training, providing insights into the model's performance and convergence.\n",
        "\n",
        "By understanding and implementing these functions, we've built a powerful text generation model capable of generating diverse and contextually relevant text based on given input. This notebook serves as a comprehensive guide to building and training Transformer-based models for text generation tasks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Code by ExzDeveloper\n",
        "\n",
        "Developer by Ezra Valen Ne Tofa\n",
        "\n",
        "Email: officialbangezz@gmail.com\n",
        "\n",
        "Github: https://github.com/exzgit\n",
        "\n",
        "Repository: https://github.com/exzgit/TAG-Model\n",
        "\n",
        "Support me: https://ko-fi.com/exzcsm\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
